# ðŸ“˜ Quick Review 

## ðŸ”¹ Preprocessing (Data Cleaning & Preparation)

| Step | What It Does | Why Itâ€™s Important | Example |
|------|---------------|--------------------|----------|
| **Handling Missing Values** | Fill or remove null values. | Missing data can bias results. | Fill with mean/median using `df.fillna()` |
| **Encoding Categorical Data** | Convert text to numbers. | Models only handle numeric input. | One-hot encoding â†’ â€œMale/Femaleâ€ â†’ [1, 0] |
| **Feature Scaling** | Normalize or standardize features. | Keeps features on same scale. | Use `StandardScaler()` or `MinMaxScaler()` |
| **Outlier Removal** | Remove extreme values. | Prevents model distortion. | Remove if Z > 3 or outside IQR |
| **Train/Test Split** | Split data into training & testing. | Avoids overfitting. | Use `train_test_split()` |
| **Balancing Data** | Fix imbalance in classification. | Prevents bias toward majority class. | Use SMOTE or undersampling |

---
## ðŸ”¹ PCC (Pearson Correlation Coefficient)
**What it does:**  
Measures the **linear relationship** between two variables.  

**Why we use it:**  
- Best when the relationship is **straight-line (linear)**  
- Sensitive to **outliers**  
- Great for small and clean datasets  

**Values:**  
- +1 â†’ strong positive  
- -1 â†’ strong negative  
- 0 â†’ no correlation  

**Example:**  
As study hours increase, exam score increases â†’ PCC â‰ˆ +1  

---

## ðŸ”¹ Partial Correlation
**What it does:**  
Finds the **true effect** of a feature after removing other featuresâ€™ influence.  

**Why we use it:**  
When PCC is high but the model is weak â†’ helps detect **hidden dependencies**.  

**Example:**  
Age and income may both correlate with sales. Partial correlation shows whether age truly affects sales after removing incomeâ€™s effect.  

---

## ðŸ”¹ Spearman Correlation
**What it does:**  
Measures **monotonic (rank-based)** relationships between variables.  

**Why we use it:**  
- Works when the relationship is **non-linear but ordered**  
- **Robust against outliers**  
- Best for **large datasets**  

**Example:**  
If higher education levels generally mean higher salaries (even if not linear), Spearman captures it.  

---

## ðŸ”¹ Kendall Correlation
**What it does:**  
Compares the **agreement between ranked pairs**.  

**Why we use it:**  
- Handles **ties (equal ranks)** better  
- Works best for **small datasets**  
- More statistically robust than Spearman but slower  

**Example:**  
Used to compare two ranking lists, like â€œperformance ranking vs. promotion orderâ€.

---

## ðŸ”¹ Heatmap
**What it does:**  
Visualizes correlations between variables using **colors**.  
- Darker â†’ stronger relationship  
- Each square = relationship between two features  

**Why we use it:**  
Quickly detect strong or weak correlations visually.  

**Example:**  
Use `sns.heatmap(df.corr(), annot=True)` to find which features are most related to the target.

---

## ðŸ”¹ Information Gain
**What it does:**  
Measures how much a feature **reduces uncertainty** about the target.  

**Why we use it:**  
- Common in **Decision Trees**  
- Great for **categorical data**  

**Example:**  
â€œIncomeâ€ gives more info about â€œPurchase Decisionâ€ than â€œAgeâ€ â†’ higher Information Gain.

---

## ðŸ”¹ Feature Ranking
**What it does:**  
Orders features based on **importance or predictive power**.  

**Why we use it:**  
- Helps in **Feature Selection**  
- Reduces model complexity  

**Common Methods:**  
`PCC`, `Spearman`, `Kendall`, `Information Gain`, `Chi-Square`, `Mutual Information`, `RFE`  

---

## ðŸ”¹ Chi-Square Test
**What it does:**  
Measures dependence between **categorical variables** and the target.  

**Why we use it:**  
To find which categorical features are **statistically related** to the output.  

**Example:**  
Testing if â€œGenderâ€ significantly affects â€œProduct Choiceâ€.

---

## ðŸ”¹ Cramerâ€™s V
**What it does:**  
Gives a **0â€“1 score** of strength between **categorical variables**.  

**Why we use it:**  
- Easier to interpret than Chi-Square  
- 0 â†’ no relation, 1 â†’ perfect relation  

**Example:**  
Cramerâ€™s V = 0.85 â†’ very strong relationship between â€œCityâ€ and â€œProduct Preferenceâ€.

---

## ðŸ”¹ Recursive Feature Elimination (RFE)
**What it does:**  
Removes **least important features** step-by-step.  

**Why we use it:**  
- Helps reduce overfitting  
- Keeps only features that improve performance  

**Example:**  
Used with Logistic Regression to remove irrelevant predictors automatically.  

---

## ðŸ”¹ Mutual Information
**What it does:**  
Measures **shared information** between feature and target.  

**Why we use it:**  
- Works with both **categorical and continuous** data  
- Detects **non-linear** relationships  

**Example:**  
â€œTime spent on siteâ€ shares more info with â€œBuy/Not Buyâ€ than â€œAgeâ€.

---

## ðŸ”¹ AUC (Area Under the ROC Curve)
**What it does:**  
Evaluates modelâ€™s **classification performance**.  

**Why we use it:**  
- Measures ability to **separate classes**  
- 1.0 = perfect model, 0.5 = random guessing  

**Example:**  
AUC = 0.9 â†’ strong model  
AUC = 0.5 â†’ weak model  

**Tip:**  
If the ROC curve is close to the top-left corner â†’ better performance.

---

## ðŸ”¹ Regression Metrics

| Metric | Description | When to Use | Example |
|--------:|--------------|-------------|----------|
| **RÂ²** | Explains how much of the variance in data the model captures. | General regression performance. | RÂ² = 0.85 â†’ model explains 85% of data behavior. |
| **Adjusted RÂ²** | Adjusted for the number of features. | When many features exist. | Higher = better fit with fewer features. |
| **MAE** | Mean Absolute Error (average error). | When all errors are equally important. | MAE = 3 â†’ avg. error = 3 units. |
| **MSE** | Mean Squared Error (squares large errors). | When large mistakes matter more. | MSE = 9 â†’ avg. squared error = 9. |
| **RMSE** | Root Mean Squared Error. | When interpretability in original units is needed. | RMSE = 3 â†’ avg. error = 3 units. |

---

## ðŸ”¹ Outlier Detection

### 1ï¸âƒ£ Visual Methods (for small/medium datasets)
- **Scatter Plot:** Isolated points = outliers.  
- **Box Plot:** Points outside whiskers = outliers.  

### 2ï¸âƒ£ Statistical Methods (for large datasets)
- **Z-Score:** Points far (e.g. > 3 std dev) from mean are outliers.  
- **IQR (Interquartile Range):** Points outside Q1â€“Q3 range = outliers.  

**Why we use them:**  
- Visual â†’ best for small data  
- Statistical â†’ better for big data  

---

## ðŸ”¹ P-Value
**What it does:**  
Checks if the result could happen **by chance**.  

**Why we use it:**  
- To test **statistical significance**  
- **p < 0.05** â†’ strong evidence against random chance  

**Example:**  
p = 0.03 â†’ statistically significant difference.

---


## ðŸ”¹ Model Evaluation Summary

| Model Type | Common Metrics | When to Use | Example |
|-------------|----------------|-------------|----------|
| **Regression** | RÂ², Adjusted RÂ², MAE, MSE, RMSE | Predicting continuous values | Predicting house prices |
| **Classification** | Accuracy, Precision, Recall, F1, AUC | Predicting categories | Spam detection |
| **Clustering** | Silhouette Score, Daviesâ€“Bouldin Index | Unsupervised tasks | Customer segmentation |

---

âœ… **Final Tip:**  
Always preprocess your data â†’ select the right features â†’ evaluate the model using the right metric.  
Good preprocessing = better performance & reliable results.

---
